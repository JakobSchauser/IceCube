{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T20:04:49.196004Z",
     "start_time": "2021-02-08T20:04:49.190022Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sqlite3, pickle, sys, gzip, shutil, time\n",
    "from tqdm import tqdm\n",
    "import os.path as osp\n",
    "\n",
    "from pandas import read_sql, concat\n",
    "from sklearn.preprocessing import normalize, RobustScaler\n",
    "from sklearn.neighbors import kneighbors_graph as knn\n",
    "\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "from spektral.data import Dataset, Graph\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T20:04:49.481850Z",
     "start_time": "2021-02-08T20:04:49.476858Z"
    }
   },
   "outputs": [],
   "source": [
    "features = [\"dom_x\", \"dom_y\", \"dom_z\", \"time\", \"charge_log10\"]\n",
    "targets  = [\"energy_log10\", \"position_x\", \"position_y\", \"position_z\", \"direction_x\", \"direction_y\", \"direction_z\"]\n",
    "\n",
    "muon = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T20:04:56.959492Z",
     "start_time": "2021-02-08T20:04:56.943490Z"
    }
   },
   "outputs": [],
   "source": [
    "class graph0(Dataset):\n",
    "    \"\"\"\n",
    "    Class for a graph dataset with the following edge features:\n",
    "    distance, time-difference and unit vector for direction\n",
    "    The node features are normalized with a robust scaler before calculating edge features\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file, n_data = None, skip = 0, n_neighbors = 6, train_val_test_split = [0.8, 0.1, 0.1], **kwargs):\n",
    "        self.n_data = n_data\n",
    "        self.skip   = skip\n",
    "        self.n_neighbors = n_neighbors\n",
    "        if sum(train_val_test_split) != 1:\n",
    "            sys.exit(\"Train, test, val ratios must add up to 1\")\n",
    "        self.train_size, self.val_size, self.test_split = train_val_test_split\n",
    "        self.seed = 25\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @property\n",
    "    def path(self):\n",
    "        \"\"\"\n",
    "        Set the path of the data to be in the processed folder\n",
    "        \"\"\"\n",
    "        cwd = osp.dirname(osp.realpath(file))\n",
    "        path = osp.join(cwd, \"processed/graph1\")\n",
    "        return path\n",
    "\n",
    "\n",
    "    def download(self):\n",
    "        download_start = time.time()\n",
    "        # Get raw_data\n",
    "        db_folder = osp.join(osp.dirname(osp.realpath(dile)), \"db_files\")\n",
    "        db_file   = osp.join(db_folder, \"rasmus_classification_muon_3neutrino_3mio.db\")\n",
    "\n",
    "        # Make output folder\n",
    "        os.mkdir(self.path)\n",
    "\n",
    "        print(\"Connecting to db-file\")\n",
    "        with sqlite3.connect(db_file) as conn:\n",
    "            # Find indices to cut after\n",
    "            try:\n",
    "                if muon:\n",
    "                    start_id = conn.execute(f\"select distinct event_no from features where event_no > 130000000 limit 1 offset {self.skip}\").fetchall()[0][0]\n",
    "                    stop_id  = conn.execute(f\"select distinct event_no from features where event_no > 130000000 limit 1 offset {self.skip + self.n_data}\").fetchall()[0][0]\n",
    "                else:\n",
    "                    start_id = conn.execute(f\"select distinct event_no from features limit 1 offset {self.skip}\").fetchall()[0][0]\n",
    "                    stop_id  = conn.execute(f\"select distinct event_no from features limit 1 offset {self.skip + self.n_data}\").fetchall()[0][0]\n",
    "            except:\n",
    "                start_id = 0\n",
    "                stop_id  = 999999999\n",
    "\n",
    "            # SQL queries format\n",
    "            feature_call = \", \".join(features)\n",
    "            target_call  = \", \".join(targets)\n",
    "\n",
    "            # Load data from db-file\n",
    "            print(\"Reading files\")\n",
    "            df_event = read_sql(f\"select event_no       from features where event_no >= {start_id} and event_no < {stop_id}\", conn)\n",
    "            df_feat  = read_sql(f\"select {feature_call} from features where event_no >= {start_id} and event_no < {stop_id}\", conn)\n",
    "            df_targ  = read_sql(f\"select {target_call } from truth    where event_no >= {start_id} and event_no < {stop_id}\", conn)\n",
    "            \n",
    "            transformers = pickle.load(open(osp.join(db_folder, \"transformers.pkl\"), 'rb'))\n",
    "            trans_x      = transformers['features']\n",
    "            trans_y      = transformers['truth']\n",
    "\n",
    "\n",
    "            for col in df_feat.columns:\n",
    "                df_feat[col] = trans_x[col].inverse_transform(np.array(df_feat[col]).reshape(1, -1)).T\n",
    "\n",
    "            for col in df_targ.columns:\n",
    "                df_targ[col] = trans_y[col].inverse_transform(np.array(df_targ[col]).reshape(1, -1)).T\n",
    "            \n",
    "            \n",
    "\n",
    "            # Cut indices\n",
    "            print(\"Splitting data to events\")\n",
    "            idx_list    = np.array(df_event)\n",
    "            x_not_split = np.array(df_feat)\n",
    "\n",
    "            _, idx, counts = np.unique(idx_list.flatten(), return_index = True, return_counts = True) \n",
    "            xs          = np.split(x_not_split, np.cumsum(counts)[:-1])\n",
    "\n",
    "            ys          = np.array(df_targ)\n",
    "\n",
    "\n",
    "\n",
    "            # Generate adjacency matrices\n",
    "            print(\"Generating adjacency matrices\")\n",
    "            graph_list = []\n",
    "            for x, y in tqdm(zip(xs, ys), total = len(xs)):\n",
    "                a = knn(x[:, :3], self.n_neighbors)\n",
    "\n",
    "                \n",
    "\n",
    "                graph_list.append(Graph(x = x, a = a, y = y))\n",
    "\n",
    "            graph_list = np.array(graph_list, dtype = object)\n",
    "\n",
    "\n",
    "            print(\"Saving dataset\")\n",
    "            pickle.dump(graph_list, open(osp.join(self.path, \"data.dat\"), 'wb'))\n",
    "\n",
    "        \n",
    "    def read(self):\n",
    "        print(\"Loading data to memory\")\n",
    "        data   = pickle.load(open(osp.join(self.path, \"data.dat\"), 'rb'))\n",
    "\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "        idxs = np.random.permutation(len(data))\n",
    "        train_split = int(self.train_size * len(data))\n",
    "        val_split   = int(self.val_size * len(data)) + train_split\n",
    "\n",
    "        idx_tr, idx_val, idx_test  = np.split(idxs, [train_split, val_split])\n",
    "        self.index_lists = [idx_tr, idx_val, idx_test]\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T20:22:40.728224Z",
     "start_time": "2021-02-08T20:22:40.623191Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-b37ecab1c392>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-87df05ca5cbc>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n_data, skip, n_neighbors, train_val_test_split, **kwargs)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spektral\\data\\dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, transforms, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;31m# Download data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mosp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-87df05ca5cbc>\u001b[0m in \u001b[0;36mpath\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mSet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mprocessed\u001b[0m \u001b[0mfolder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \"\"\"\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mcwd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mosp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mosp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mosp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"processed/graph1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = graph0(n_data = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Removing current data folder\")\n",
    "    \n",
    "    path = osp.dirname(osp.realpath(__file__))\n",
    "    if not \"processed\" in os.listdir(path):\n",
    "        os.mkdir(osp.join(path, \"processed\"))\n",
    "    if not \"raw_files\" in os.listdir(path):\n",
    "        os.mkdir(osp.join(path, \"processed\"))\n",
    "        print(\"Folder created for raw files, please add some before continuing\")\n",
    "        sys.exit()\n",
    "\n",
    "    if os.path.isdir(osp.join(path, \"processed\", \"graph_w_edge0\")):\n",
    "        shutil.rmtree(osp.join(path, \"processed\", \"graph_w_edge0\"))\n",
    "    if len(sys.argv) == 2:\n",
    "        n_data = int(sys.argv[1])\n",
    "        print(f\"Preparing dataset with {n_data} graphs\")\n",
    "    else:\n",
    "        n_data = None\n",
    "        print(\"Preparing dataset with all availible raw data\")\n",
    "\n",
    "    # Preparing data \n",
    "    dataset = graph_w_edge2(n_data = n_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T20:00:50.815234Z",
     "start_time": "2021-02-08T20:00:50.810211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_devices = tf.config.list_physical_devices('GPU') \n",
    "len(gpu_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T20:37:20.285145Z",
     "start_time": "2021-02-08T20:37:20.279161Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
